{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(file_path):\n",
    "    df = pd.read_pickle(file_path)\n",
    "\n",
    "    X = pd.concat([df['enrollment'],\n",
    "                   df['description_embedding'].apply(pd.Series),\n",
    "                   df['inclusion_embedding'].apply(pd.Series),\n",
    "                   df['exclusion_embedding'].apply(pd.Series),\n",
    "                   df['treatment_embedding'].apply(pd.Series),\n",
    "                   df['disease_embedding'].apply(pd.Series),\n",
    "                   df['measures_embedding'].apply(pd.Series),\n",
    "                   df['timeframes_embedding'].apply(pd.Series)], axis=1)\n",
    "    y = df['durationMonths']\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_dataset('./data_example/train_df.pkl')\n",
    "X_test, y_test = prepare_dataset('./data_example/test_df.pkl')\n",
    "X_incompleted, y_incompleted = prepare_dataset('./data_example/incompleted_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize enrollment\n",
    "scaler = StandardScaler()\n",
    "X_train_enroll_scaled = scaler.fit_transform(X_train[['enrollment']])\n",
    "X_train_scaled = np.concatenate([X_train_enroll_scaled, X_train.iloc[:, 1:].values], axis=1)\n",
    "\n",
    "X_test_enroll_scaled = scaler.transform(X_test[['enrollment']])\n",
    "X_test_scaled = np.concatenate([X_test_enroll_scaled, X_test.iloc[:, 1:].values], axis=1)\n",
    "\n",
    "X_incompleted_enroll_scaled = scaler.transform(X_incompleted[['enrollment']])\n",
    "X_incompleted_scaled = np.concatenate([X_incompleted_enroll_scaled, X_incompleted.iloc[:, 1:].values], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling training\n",
    "model_ridge = Ridge()\n",
    "param_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_ridge,\n",
    "    param_grid=param_grid, cv=5,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_model_ridge = grid_search.best_estimator_\n",
    "\n",
    "# Evaluation\n",
    "y_pred_ridge = best_model_ridge.predict(X_test_scaled)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model_rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Tune the parameter\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_rf,\n",
    "    param_distributions=param_grid, \n",
    "    n_iter=10, cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "best_model_rf = random_search.best_estimator_\n",
    "\n",
    "# Evaluation\n",
    "y_pred_rf = best_model_rf.predict(X_test_scaled)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hongxiang/Desktop/MADS/Milestone III/TrialDuraPredict/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost regressor\n",
    "model_xgb = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Tune the parameter\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_xgb,\n",
    "    param_distributions=param_grid, \n",
    "    n_iter=10, cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "best_model_xgb = random_search.best_estimator_\n",
    "\n",
    "# Evaluation\n",
    "y_pred_xgb = best_model_xgb.predict(X_test_scaled)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an explainer object using the XGBoost model\n",
    "explainer = shap.Explainer(best_model_xgb)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate SHAP values for groups of features\n",
    "sections = ['enrollment', 'description', 'inclusion', 'exclusion',\n",
    "            'treatment', 'disease', 'measure', 'timeframe']\n",
    "\n",
    "section_shap_values = {}\n",
    "\n",
    "for i, section in enumerate(sections):\n",
    "    if section == 'enrollment':\n",
    "        section_shap_value = shap_values.values[:, 0]\n",
    "    else:\n",
    "        start_idx = (i-1) * 768 + 1\n",
    "        end_idx = start_idx + 768\n",
    "        section_shap_value = np.sum(shap_values.values[:, start_idx:end_idx], axis=1)\n",
    "    \n",
    "    section_shap_values[section] = section_shap_value\n",
    "\n",
    "section_shap_values = pd.DataFrame(section_shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model_ffnn = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_ffnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define callbacks (optional but recommended for saving the best model)\n",
    "checkpoint = ModelCheckpoint('./results/ffnn_model.keras', monitor='val_loss',\n",
    "                             save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model_ffnn.fit(X_train_scaled, y_train,\n",
    "               epochs=50, batch_size=32,\n",
    "               validation_data=(X_test_scaled, y_test),\n",
    "               callbacks=[checkpoint])\n",
    "\n",
    "# Load the best saved model\n",
    "model_ffnn = load_model('./results/ffnn_model.keras')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_ffnn = model_ffnn.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_cnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define callbacks (optional)\n",
    "checkpoint = ModelCheckpoint('./results/cnn_model.keras', monitor='val_loss',\n",
    "                             save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model_cnn.fit(X_train_scaled, y_train,\n",
    "              epochs=50, batch_size=32,\n",
    "              validation_data=(X_test_scaled, y_test),\n",
    "              callbacks=[checkpoint])\n",
    "\n",
    "# Load the best saved model\n",
    "model_cnn = load_model('./results/cnn_model.keras')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_cnn = model_cnn.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model_lstm = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define callbacks (optional)\n",
    "checkpoint = ModelCheckpoint('./results/lstm_model.keras', monitor='val_loss',\n",
    "                             save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model_cnn.fit(X_train_scaled, y_train,\n",
    "              epochs=50, batch_size=32,\n",
    "              validation_data=(X_test_scaled, y_test),\n",
    "              callbacks=[checkpoint])\n",
    "\n",
    "# Load the best saved model\n",
    "model_cnn = load_model('./results/lstm_model.keras')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_cnn = model_cnn.predict(X_test_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
